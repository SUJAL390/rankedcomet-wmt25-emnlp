# rankedCOMET – WMT 2025 QE Submission

This repository contains the full rankedCOMET system described in the WMT 2025 Task 1 Quality Estimation sprint paper. It packages the inference notebook, calibration scripts, diagnostics, figures, and ablations required to reproduce the reported results and analyses.

## 🚀 Quick start

1. **Clone the repo**
	```bash
	git clone https://github.com/SUJAL390/rankedcomet-wmt25-emnlp.git
	cd rankedcomet-wmt25-emnlp
	```
2. **Create a Python environment (≥3.9 recommended)**
	```bash
	python -m venv .venv
	.venv\Scripts\activate  # PowerShell
	```
3. **Install dependencies**
	```bash
	pip install -r requirements.txt
	```
4. **Stage required data**
	Place the following files in the repository root (not distributed here):
	- `mteval-task1-test25.tsv.gz` – official WMT2025 Task 1 test set.
	- `raw_scores_backup.pkl` – cached COMET raw scores (generated by the notebook below).

> **Hardware note:** COMET inference benefits from a CUDA-capable GPU. All remaining scripts are CPU-friendly.

## 📁 Repository structure

```
.
├── notebooks/
│   └── wmt25-task1-qualityprediction-sprint2.ipynb   # COMET inference + submission writer
├── scripts/
│   ├── calibrate_scores.py                           # Per-language rank→min–max calibration
│   ├── final_gambit.py                               # Negative ensemble experiment
│   ├── compare_raw_ranked.py                         # Raw vs ranked correlation diagnostics
│   ├── analysis_per_language.py                      # Pearson/Spearman/Kendall + bootstrap CIs
│   ├── rankedcomet_full_analysis.py                  # Variance analysis, dev mapping, ablations
│   ├── rankedcomet_figures.py                        # Figures A/B/C from the paper
│   └── create_variance_plot.py                       # Camera-ready variance scatter
├── figures_camera_ready/                             # Publication-ready figures
├── figures1/                                         # Intermediate diagnostic figures
├── results_all/                                      # CSV outputs (variance, ablation, etc.)
├── segments.tsv                                      # Ranked submission (produced by calibration)
├── raw_scores_backup.pkl                             # Cached COMET raw scores (not versioned)
├── requirements.txt
└── README.md
```

## 🧭 Reproduction workflow

1. **Generate raw COMET scores**
	- Open `notebooks/wmt25-task1-qualityprediction-sprint2.ipynb` in VS Code or Jupyter.
	- Update the paths if your test file is stored elsewhere.
	- Run all cells to create `segments.tsv` and cache raw scores into `raw_scores_backup.pkl`.

2. **Per-language calibration (rankedCOMET)**
	```bash
	python scripts/calibrate_scores.py
	```
	- Produces `segments_ranked_final_v2.tsv`, the calibrated submission described in the paper.

3. **Negative ensemble attempt** (optional, reported as negative result)
	```bash
	python scripts/final_gambit.py
	```

4. **Raw vs ranked diagnostics**
	```bash
	python scripts/compare_raw_ranked.py segments_ranked_final_v2.tsv raw_scores_backup.pkl
	```
	- Outputs `raw_vs_ranked_stats.csv` used in Table 3 / Figure 2.

5. **Variance analysis, dev-based mapping, and ablations**
	```bash
	# Variance analysis (writes results_all/var_analysis/*)
	python scripts/rankedcomet_full_analysis.py variance-analysis \
		 --test segments_ranked_final_v2.tsv \
		 --raw raw_scores_backup.pkl \
		 --outdir results_all/var_analysis

	# Dev-calibration (optional; requires dev files)
	python scripts/rankedcomet_full_analysis.py dev-calibrate \
		 --dev_tsv DEV_SEGMENTS.tsv \
		 --dev_raw DEV_RAW.pkl \
		 --test_tsv mteval-task1-test25.tsv \
		 --test_raw raw_scores_backup.pkl \
		 --out_prefix devmap \
		 --outdir results_all/dev_calibrate

	# Ablation suite
	python scripts/rankedcomet_full_analysis.py ablation \
		 --test segments_ranked_final_v2.tsv \
		 --raw raw_scores_backup.pkl \
		 --outdir results_all/ablation
	```

6. **Figures**
	```bash
	python scripts/create_variance_plot.py \
		 --test-tsv segments_ranked_final_v2.tsv \
		 --raw-pkl raw_scores_backup.pkl \
		 --output figures_camera_ready/var_rank_vs_var_raw_final.png

	python scripts/rankedcomet_figures.py --fig-delta-var --fig-hist --fig-ties \
		 --segments segments_ranked_final_v2.tsv \
		 --raw raw_scores_backup.pkl \
		 --outdir figures_camera_ready
	```

7. **Per-language correlation tables**
	```bash
	python scripts/analysis_per_language.py segments_ranked_final_v2.tsv --bootstrap 2000
	```
	- Produces `per_language_correlations.csv`, `per_language_pearson_bootstrap.csv`, and `aggregated_results.csv`.

## 🧪 Mapping outputs to paper artifacts

| Paper artifact | How to reproduce |
| --- | --- |
| Ranked submission file | `scripts/calibrate_scores.py` → `segments_ranked_final_v2.tsv` |
| Table 1 snapshot (Codabench) | Provided for context – generated externally (Codabench UI) |
| Table 2 (variance diagnostics) | `rankedcomet_full_analysis.py variance-analysis` |
| Table 3 (raw vs ranked correlations) | `compare_raw_ranked.py` |
| Table 4 (ablations) | `rankedcomet_full_analysis.py ablation` |
| Figure 1 | `create_variance_plot.py` |
| Figure 2 & Figure 3 | `rankedcomet_figures.py` |

## 📦 Dataset availability

- **Test set (`mteval-task1-test25.tsv.gz`)** – distributed by the WMT 2025 QE task organizers on Codabench. Obtain it under the shared task rules.
- **Dev set** – optional for dev-based calibration; follow WMT data usage guidelines.
- **Human annotations** – required for running `analysis_per_language.py`. Provide a TSV with either `human`, `mqm`, or equivalent column as described in the script help message.

## ⚠️ Limitations

- The reported leaderboard position references the preliminary Codabench snapshot. Final official rankings may differ.
- Rank-based calibration cannot recover signal for language pairs where the base COMET model has none (e.g., EN–AR/EN–BHO in our experiments).
- When test-set statistics are disallowed, rely on the dev-based mapping provided in the scripts.

